import os
import base64
import sys
import json
import datetime
import uuid

from abc import ABC, abstractclassmethod
from pathlib import Path
from io import BytesIO

from openai import AzureOpenAI
from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure.ai.documentintelligence.models import AnalyzeDocumentRequest
from azure.core.credentials import AzureKeyCredential
import pdfplumber
from langchain_text_splitters import RecursiveCharacterTextSplitter

from blob_utils.blob_utils import upload_blob, download_blob, ensure_folder_exists

class BaseConverter(ABC):
    def __init__(self, 
                 container_name: str,
                 blob_src_path: str | Path,
                 blob_dst_path: str | Path,
                 ):
        super().__init__()
        if not isinstance(container_name, str):
            raise TypeError(f"container_name must be str, got {type(container_name)}")
        if not isinstance(blob_src_path, (str, Path)):
            raise TypeError(f"blob_src_path must be str or Path, got {type(blob_src_path)}")
        if not isinstance(blob_dst_path, (str, Path)):
            raise TypeError(f"blob_dst_path must be str or Path, got {type(blob_dst_path)}")
        
        self.container_name = container_name
        self.blob_src_path = blob_src_path
        self.blob_dst_path = blob_dst_path
    
    @classmethod
    @abstractclassmethod
    def convert_and_upload(self):
        pass

class DocumentIntelligenceConverter(BaseConverter):
    """Convert documents (PDFs) to structured text/JSON using Azure Document Intelligence,
    then upload the results back to blob storage.

    Args:
        container_name (str): The name of the blob container.
        blob_src_path (str|Path): The source blob path, e.g. "src/dir/file.pdf".
        blob_dst_path (str|Path): The destination blob path, e.g. "dst/dir/file.json".
        model_name (str): The name of the Document Intelligence model (default: "prebuilt-layout").
    """
     
    def __init__(self, container_name: str, blob_src_path: str | Path, blob_dst_path: str | Path, 
                 model_name: str = "prebuilt-layout"):
        super().__init__(container_name, blob_src_path, blob_dst_path)
        self.model_name = model_name
        self.endpoint = os.environ["DOCUMENT_INTELLIGENCE_ENDPOINT"]
        self.key = os.environ["DOCUMENT_INTELLIGENCE_KEY"]
        if not self.endpoint or not self.key:
            raise ValueError("Missing environment variables for Document Intelligence endpoint or key.")
    
    def convert_and_upload(self):
        """Download PDF from blob storage, analyze it with Document Intelligence,
        and upload the extracted structured data (as JSON)."""

        file_bytes = download_blob(self.container_name, self.blob_src_path, None)
        # Get Document Intelligence result
        converted_bytes = self._custom_converter(file_bytes)
        upload_blob(
            blob_container_name=self.container_name,
            file_src_path=None,
            file_src_bytes=converted_bytes,
            blob_dst_path=Path(self.blob_dst_path),
            metadata={
                "converted": "true",
                "model": self.model_name
            }
        )
        print(f"✅ Uploaded JSON → {Path(self.blob_dst_path)}")


    def _custom_converter(self,pdf_bytes: bytes) -> str:
        """Perform document analysis using Azure Document Intelligence."""
        
        client = DocumentIntelligenceClient(
            endpoint=self.endpoint,
            credential=AzureKeyCredential(self.key)
        )
        poller = client.begin_analyze_document(
            self.model_name,
            AnalyzeDocumentRequest(bytes_source=pdf_bytes),
            features=["ocrHighResolution"],
            output_content_format="markdown"
        )
        result = poller.result()

        json_bytes = json.dumps(
            result.as_dict(),
            ensure_ascii=False,
            indent=2
        ).encode("utf-8")

        return json_bytes

class JsonChunkConverter(BaseConverter):
    """Convert a JSON file containing 'content' field into multiple chunked JSON files.

    Uses LangChain's RecursiveCharacterTextSplitter to intelligently split text
    into semantically coherent chunks.

    Each chunk will be saved as a separate JSON file containing:
        - id: generated by ISO8601 timestamp + UUID
        - child_chunk: the chunk text

    Args:
        container_name (str): Blob container name.
        blob_src_path (str|Path): Source blob path (JSON file with 'content' field).
        blob_dst_dir (str|Path): Destination directory in blob storage for output chunks.
        chunk_size (int): Max characters per chunk (default: 300).
        chunk_overlap (int): Overlapping chars between chunks (default: 50).
    """
    def __init__(
        self,
        container_name: str,
        blob_src_path: str | Path,
        blob_dst_dir: str | Path,
        chunk_size: int = 100,
        chunk_overlap: int = 20
    ):
        super().__init__(container_name, blob_src_path, blob_dst_dir)
        self.blob_dst_dir = blob_dst_dir
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
    
    def convert_and_upload(self):
        file_bytes = download_blob(self.container_name, self.blob_src_path, None)
        data = json.loads(file_bytes.decode("utf-8"))
        content = data.get("content", "").strip()
        if not content:
            raise ValueError(f"No 'content' found in {self.blob_src_path}")
        
        chunks = self._custom_converter(content)
        now_str = datetime.datetime.now(datetime.UTC).strftime("%Y-%m-%dT%H:%M:%SZ")
        for i, chunk_text in enumerate(chunks, start=1):
            chunk_id = f"{now_str}_{uuid.uuid4()}"
            chunk_data = {"id": chunk_id, "child_chunk": chunk_text}

            blob_dst_path = Path(self.blob_dst_dir) / f"{Path(self.blob_src_path).stem}_chunk_{i:04d}.json"

            upload_blob(
                blob_container_name=self.container_name,
                file_src_path=None,
                file_src_byte=json.dumps(chunk_data, ensure_ascii=False, indent=2).encode("utf-8"),
                blob_dst_path=blob_dst_path,
                metadata={
                    "converted": "true",
                    "splitter": "langchain",
                    "chunk_size": str(self.chunk_size)
                }
            )


    def _custom_converter(self, text: str) -> list[str]:
        """Perform intelligent chunking using LangChain RecursiveCharacterTextSplitter."""
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            separators=["\n##", "\n###", "\n", ".", " ", "。"]
        )
        chunks = splitter.split_text(text)
        return chunks